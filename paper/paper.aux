\relax 
\citation{pearson1901}
\citation{serovich87}
\citation{turk91}
\citation{shlens09}
\citation{borja09}
\citation{shlens09}
\citation{borja09}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Image reconstruction with PCA}{1}}
\citation{borja09}
\citation{borja09}
\citation{borja09}
\@writefile{toc}{\contentsline {section}{\numberline {III}Classification using reconstruction}{2}}
\newlabel{weighted_total_error}{{6}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{2}}
\citation{cbcl}
\citation{jiang09}
\citation{code}
\bibstyle{IEEEtran}
\bibdata{paper}
\bibcite{sibgrapi2010}{1}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of area under the curve (AUC) of the classifiers based on grayscale error, edge error, and total error, respectively, for many values of $k$. The AUCs of the edge-based classifier are always larger then the grayscale-based classifier. This indicates edge errors are more relevant to the classification.}}{4}}
\newlabel{table-gray-vs-edge}{{I}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Weights that produced best classification rates for each value of $k$. Note that the weights associated to edge errors are usually larger than the errors associated to grayscale errors, what reinforces the greater importance of the edge errors to the classification.}}{4}}
\newlabel{table-weights}{{II}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
